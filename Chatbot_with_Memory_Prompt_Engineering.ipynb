{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Gemini Chatbot with Memory (Colab Example)\n",
        "\n",
        "This project demonstrates how to build an **interactive chatbot with memory** using the **Google Gemini API** (`google-generativeai`).  \n",
        "The chatbot keeps track of the conversation state, allowing for contextual and continuous dialogue.\n",
        "\n",
        "---\n",
        "\n",
        "## Features\n",
        "- Uses **Gemini 2.0 Flash** model for fast responses.  \n",
        "- Maintains **chat history** for contextual answers.  \n",
        "- Simple interactive loop in Colab/terminal.  \n",
        "- Shows memory state after each turn for debugging.\n",
        "\n",
        "---\n",
        "\n",
        "## Installation\n",
        "Install the required dependency:\n",
        "\n",
        "```bash\n",
        "pip install google-generativeai\n"
      ],
      "metadata": {
        "id": "Ae46rtsMGrrD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZZhaa1JnEBE"
      },
      "outputs": [],
      "source": [
        "# ==============================\n",
        "# Step 1: Install dependencies\n",
        "# ==============================\n",
        "!pip install google-generativeai --quiet\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Step 2: Import Libraries\n",
        "# ==============================\n",
        "import google.generativeai as genai\n",
        "import textwrap\n"
      ],
      "metadata": {
        "id": "Bn5aP1yD5M0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Step 3: Configure API Key directly\n",
        "# ==============================\n",
        "# API_KEY = \"\"  # your key here\n",
        "# genai.configure(api_key=API_KEY)\n",
        "#           (or)\n",
        "# ==============================\n",
        "# Step 3: Configure API Key thru Secrets\n",
        "# ==============================\n",
        "from google.colab import userdata   # for fetching secret key\n",
        "\n",
        "# Retrieve API key stored in Colab's secrets\n",
        "API_KEY = userdata.get('GEMINI_API_KEY')   # make sure you saved it with this name\n",
        "genai.configure(api_key=API_KEY)\n"
      ],
      "metadata": {
        "id": "dDrB29Sh5U5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Step 4: Create Chatbot with Memory\n",
        "# ==============================\n",
        "# Chat session will hold memory (state)\n",
        "model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
        "chat = model.start_chat(history=[])\n"
      ],
      "metadata": {
        "id": "bAXCE1kH5qWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Step 5: Helper to Display Messages\n",
        "# ==============================\n",
        "def pretty_print(text):\n",
        "    print(\"\\n\".join(textwrap.wrap(text, width=100)))\n"
      ],
      "metadata": {
        "id": "ukA7sLh35uAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Step 6: Start Interactive Chat\n",
        "# ==============================\n",
        "print(\" Gemini Chatbot with Memory & State\")\n",
        "print(\"Type 'exit' to stop.\\n\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
        "        print(\"Bot: Goodbye! \")\n",
        "        break\n",
        "\n",
        "    response = chat.send_message(user_input)\n",
        "    bot_reply = response.text\n",
        "\n",
        "    print(\"Bot:\", end=\" \")\n",
        "    pretty_print(bot_reply)\n",
        "\n",
        "    # Debug: Show memory state\n",
        "    print(\"\\n[Memory State So Far]\")\n",
        "    for turn in chat.history:\n",
        "      role = turn.role\n",
        "    # parts is a list of Part objects, so we join their text\n",
        "      parts_text = \" \".join([p.text for p in turn.parts if hasattr(p, \"text\")])\n",
        "    print(f\"{role}: {parts_text}\")\n",
        "    print(\"-\" * 60)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "9lecRKkE5xWr",
        "outputId": "1b10b0f7-72da-4b81-c858-266270ac32e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Gemini Chatbot with Memory & State\n",
            "Type 'exit' to stop.\n",
            "\n",
            "You: my name is saravanan\n",
            "Bot: Nice to meet you, Saravanan!\n",
            "\n",
            "[Memory State So Far]\n",
            "model: Nice to meet you, Saravanan!\n",
            "\n",
            "------------------------------------------------------------\n",
            "You: tell about you\n",
            "Bot: I am a large language model, trained by Google.  I don't have a body, a personal history, or\n",
            "emotions.  My purpose is to process information and respond to a wide range of prompts and\n",
            "questions.  I can provide information on many topics, translate languages, write different kinds of\n",
            "creative content, and answer your questions in an informative way, even if they are open ended,\n",
            "challenging, or strange.  Think of me as a very advanced search engine combined with a powerful\n",
            "writing tool. I learn from the massive dataset I was trained on, which includes a huge amount of\n",
            "text and code.  I don't have experiences like humans do; my knowledge comes from the data I've\n",
            "processed.  I'm always learning and improving, but I'm not perfect.  Sometimes I might make mistakes\n",
            "or give inaccurate information.  It's always a good idea to double-check important information with\n",
            "other reliable sources.\n",
            "\n",
            "[Memory State So Far]\n",
            "model: I am a large language model, trained by Google.  I don't have a body, a personal history, or emotions.  My purpose is to process information and respond to a wide range of prompts and questions.  I can provide information on many topics, translate languages, write different kinds of creative content, and answer your questions in an informative way, even if they are open ended, challenging, or strange.\n",
            "\n",
            "Think of me as a very advanced search engine combined with a powerful writing tool. I learn from the massive dataset I was trained on, which includes a huge amount of text and code.  I don't have experiences like humans do; my knowledge comes from the data I've processed.\n",
            "\n",
            "I'm always learning and improving, but I'm not perfect.  Sometimes I might make mistakes or give inaccurate information.  It's always a good idea to double-check important information with other reliable sources.\n",
            "\n",
            "------------------------------------------------------------\n",
            "You: tell my name\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 657.98ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bot: Your name is Saravanan.\n",
            "\n",
            "[Memory State So Far]\n",
            "model: Your name is Saravanan.\n",
            "\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Gemini Chain-of-Thought Reasoning Agent (Colab)\n",
        "\n",
        "This project demonstrates how to build an **interactive reasoning assistant** powered by **Google Gemini 2.0 Flash**.  \n",
        "The agent is designed to **show its step-by-step reasoning** before giving a final concise answer.\n",
        "\n",
        "---\n",
        "\n",
        "##  Features\n",
        "- Uses **Gemini 1.5 Flash** (fast + efficient).  \n",
        "- Implements **Chain-of-Thought prompting** for transparent reasoning.  \n",
        "- Includes **retry logic** (via `tenacity`) to handle API rate limits.  \n",
        "- Reads **API key from Colab secrets** (safer than hardcoding).  \n",
        "- Interactive loop for asking multiple questions.\n",
        "\n",
        "---\n",
        "\n",
        "##  Installation\n",
        "Install required dependencies inside Colab:\n",
        "\n",
        "```bash\n",
        "pip install -q google-generativeai tenacity\n"
      ],
      "metadata": {
        "id": "pAeIS-puE6qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Step 1: Install Gemini SDK\n",
        "# ==============================\n",
        "!pip install -q google-generativeai tenacity\n",
        "\n"
      ],
      "metadata": {
        "id": "JlU34_Wb7THN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Step 2: Imports\n",
        "# ==============================\n",
        "import os\n",
        "import textwrap\n",
        "import google.generativeai as genai\n",
        "from tenacity import retry, wait_random_exponential, stop_after_attempt\n"
      ],
      "metadata": {
        "id": "dFEJU9tX6lYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Step 3: Load Gemini API Key from Colab secrets\n",
        "# ==============================\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    GEMINI_API_KEY = userdata.get(\"GEMINI_API_KEY\")  # must be set in Colab secrets\n",
        "except ImportError:\n",
        "    raise ImportError(\"This code requires Google Colab with secrets set.\")\n",
        "\n",
        "if not GEMINI_API_KEY:\n",
        "    raise ValueError(\"GEMINI_API_KEY not found in Colab secrets. Please set it first.\")\n",
        "\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n"
      ],
      "metadata": {
        "id": "HDGhgbG_6p3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Step 4: Initialize Gemini model\n",
        "# ==============================\n",
        "model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
        "\n",
        "# Retry logic for rate limits\n",
        "@retry(wait=wait_random_exponential(multiplier=1, max=60), stop=stop_after_attempt(5))\n",
        "def ask_gemini(prompt: str):\n",
        "    response = model.generate_content(prompt)\n",
        "    return response.text\n",
        "\n",
        "def cot_agent_chain_of_thought(user_query: str) -> str:\n",
        "    # Enhanced CoT prompt\n",
        "    prompt = (\n",
        "        \"You are an expert reasoning assistant.\\n\"\n",
        "        \"When solving problems, follow these steps:\\n\"\n",
        "        \"1. Restate the question in your own words.\\n\"\n",
        "        \"2. Break the problem into smaller sub-steps.\\n\"\n",
        "        \"3. Think step by step, showing all reasoning explicitly.\\n\"\n",
        "        \"4. If math is needed, show calculations.\\n\"\n",
        "        \"5. If logic or assumptions are used, state them clearly.\\n\"\n",
        "        \"6. At the end, provide a concise final answer, starting with 'Answer:'.\\n\\n\"\n",
        "        f\"Problem: {user_query}\\n\\n\"\n",
        "        \"Begin reasoning:\\n\"\n",
        "    )\n",
        "    return ask_gemini(prompt)\n",
        "\n"
      ],
      "metadata": {
        "id": "2ajQj75W6uIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Step 5: Run interactive loop\n",
        "# ==============================\n",
        "if __name__ == \"__main__\":\n",
        "    print(\" Gemini CoT Reasoning Agent\")\n",
        "    while True:\n",
        "        query = input(\"\\nEnter your question (or 'exit' to quit): \")\n",
        "        if query.strip().lower() == \"exit\":\n",
        "            print(\"Exiting. Happy reasoning! \")\n",
        "            break\n",
        "        try:\n",
        "            output = cot_agent_chain_of_thought(query)\n",
        "            print(\"\\n---\\n\" + output + \"\\n---\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to get response: {e}\")\n",
        "            print(\"Try again later or check quota/billing.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3huJquKG60Jl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
